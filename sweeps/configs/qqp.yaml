program: sweeps/finetune_sweep_wrapper.py
project: babylm2-finetune-sweeps
name: finetune-sweep-qqp
method: bayes
metric:
  goal: maximize
  name: eval/f1
command:
  - ${env}
  - python
  - ${program}
  - ../baby-llama2/results/SmolLlama-345M-2_teachers/
  - sweeps/results/SmolLlama-345M-2_teachers/
  - ${args}
parameters:
  task:
    value: qqp
  learning_rate:
    distribution: log_normal
    mu: -11
    sigma: 1
  batch_size:
    values: [1, 32]
  num_train_epochs:
    value: 1
  weight_decay:
    distribution: log_normal
    mu: 0
    sigma: 1.5
  lr_scheduler_type:
    value: "linear"
  warmup_steps:
    value: 500
  seed:
    value: 12