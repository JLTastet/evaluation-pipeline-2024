program: sweeps/finetune_sweep_wrapper.py
project: babylm2-finetune-sweeps
name: finetune-sweep-cola
method: bayes
metric:
  goal: maximize
  name: eval/mcc
early_terminate:
  type: hyperband
  min_iter: 1
  eta: 2
command:
  - ${env}
  - python
  - ${program}
  - ../baby-llama2/results/SmolLlama-345M-2_teachers/
  - sweeps/results/SmolLlama-345M-2_teachers/
  - ${args}
parameters:
  task:
    value: cola
  learning_rate:
    distribution: log_normal
    mu: -9.9
    sigma: 1
  batch_size:
    distribution: q_log_uniform_values
    min: 1
    max: 32
  num_train_epochs:
    value: 10
  weight_decay:
    distribution: log_normal
    mu: 0
    sigma: 1.5
  lr_scheduler_type:
    values: ["linear", "cosine", "constant"]
  warmup_steps:
    distribution: q_log_normal
    mu: 5.7
    sigma: 1
  seed:
    value: 12